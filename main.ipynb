{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RrzVOGWJBuVM",
        "outputId": "86dda096-fbd9-4352-d78e-335fafc2bf99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "n7KJt3Sw0ARq",
        "outputId": "ec20ef8d-450c-4352-ba2c-5f1358610716"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms, models\n",
        "\n",
        "\n",
        "class CUB200Dataset(Dataset):\n",
        "    def __init__(self, root_dir, split='train', transform=None, apply_bg_removal=False):\n",
        "        self.root_dir = root_dir\n",
        "        self.split = split\n",
        "        self.transform = transform or self.default_transform()\n",
        "        self.apply_bg_removal = apply_bg_removal\n",
        "\n",
        "        # Load metadata\n",
        "        self.data = self.load_metadata()\n",
        "\n",
        "    def load_metadata(self):\n",
        "        split_file = os.path.join(self.root_dir, f'{self.split}.txt')\n",
        "        data = pd.read_csv(split_file, sep=' ', names=['filename', 'label'])\n",
        "        data['filepath'] = data['filename'].apply(lambda x: os.path.join(self.root_dir, self.split, x))\n",
        "        return data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.data.loc[idx, 'filepath']\n",
        "        image = self.load_image(img_name)\n",
        "\n",
        "        # Apply transformations to convert the cropped image to a tensor\n",
        "        image = self.transform(image)\n",
        "\n",
        "        label = self.data.loc[idx, 'label']\n",
        "        return image, label\n",
        "\n",
        "    @staticmethod\n",
        "    def load_image(image_path):\n",
        "        try:\n",
        "            return Image.open(image_path).convert('RGB')\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading image {image_path}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def default_transform(self):\n",
        "        return transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "UYskNTDu-grs"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "from torchvision import transforms, models\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "\n",
        "# Define transformations \n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "test_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sz6EKXfc-j6x",
        "outputId": "89334074-06de-48be-812a-cafc301200cf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/efficientnet_b5_lukemelas-1a07897c.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b5_lukemelas-1a07897c.pth\n",
            "100%|██████████| 117M/117M [00:00<00:00, 204MB/s] \n"
          ]
        }
      ],
      "source": [
        "train_dataset = CUB200Dataset(root_dir='drive/MyDrive/COS30082_preprocessed', split='train', transform=train_transforms)\n",
        "test_dataset = CUB200Dataset(root_dir='drive/MyDrive/COS30082_preprocessed', split='test', transform=test_transforms)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=4)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=4)\n",
        "\n",
        "class BirdClassifier(nn.Module):\n",
        "    def __init__(self, model_name='efficientnet_b5', num_classes=200):\n",
        "        super(BirdClassifier, self).__init__()\n",
        "        \n",
        "        # Choose the model based on the input argument 'model_name'\n",
        "        if model_name == 'resnet50':\n",
        "            self.model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
        "            in_features = self.model.fc.in_features\n",
        "            self.model.fc = nn.Sequential(\n",
        "                nn.Dropout(0.5),\n",
        "                nn.Linear(in_features, num_classes)\n",
        "            )\n",
        "        \n",
        "        elif model_name == 'resnext50_32x4d':\n",
        "            self.model = models.resnext50_32x4d(weights=models.ResNeXt50_32X4D_Weights.DEFAULT)\n",
        "            in_features = self.model.fc.in_features\n",
        "            self.model.fc = nn.Sequential(\n",
        "                nn.Dropout(0.5),\n",
        "                nn.Linear(in_features, num_classes)\n",
        "            )\n",
        "        \n",
        "        elif model_name == 'efficientnet_b0':\n",
        "            self.model = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.DEFAULT)\n",
        "            in_features = self.model.classifier[1].in_features\n",
        "            self.model.classifier = nn.Sequential(\n",
        "                nn.Dropout(0.5),\n",
        "                nn.Linear(in_features, num_classes)\n",
        "            )\n",
        "        \n",
        "        elif model_name == 'efficientnet_b5':\n",
        "            self.model = models.efficientnet_b5(weights=models.EfficientNet_B5_Weights.DEFAULT)\n",
        "            in_features = self.model.classifier[1].in_features\n",
        "            self.model.classifier = nn.Sequential(\n",
        "                nn.Dropout(0.5),\n",
        "                nn.Linear(in_features, num_classes)\n",
        "            )\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported model_name: {model_name}. Choose from 'resnet50', 'resnext50_32x4d', 'efficientnet_b0', 'efficientnet_b5'.\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "    \n",
        "\n",
        "# 5. Setup Model, Criterion, Optimizer, and Scheduler\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# To use EfficientNet B5\n",
        "model = BirdClassifier(model_name='efficientnet_b5', num_classes=200)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "scheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class EarlyStopping:\n",
        "    def __init__(self, patience=5, delta=0):\n",
        "        self.patience = patience  # How many epochs to wait after last improvement\n",
        "        self.delta = delta  # Minimum change to qualify as an improvement\n",
        "        self.best_loss = None  # Best validation loss seen so far\n",
        "        self.counter = 0  # Tracks how long since the last improvement\n",
        "        self.early_stop = False  # Flag to indicate whether training should stop\n",
        "\n",
        "    def __call__(self, val_loss):\n",
        "        if self.best_loss is None:\n",
        "            self.best_loss = val_loss\n",
        "        elif val_loss > self.best_loss - self.delta:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_loss = val_loss\n",
        "            self.counter = 0  # Reset counter if validation loss improves\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 6. Training and Evaluation Functions\n",
        "def train(model, loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total_images = 0\n",
        "\n",
        "    for images, labels in tqdm(loader):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        total_images += labels.size(0)\n",
        "\n",
        "    train_accuracy = (correct / total_images) * 100\n",
        "    return running_loss / len(loader.dataset), train_accuracy\n",
        "\n",
        "def evaluate(model, loader, criterion, device, num_classes=200):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct_top1 = 0\n",
        "    total_images = 0\n",
        "\n",
        "    # Track per-class accuracy\n",
        "    class_correct = torch.zeros(num_classes).to(device)\n",
        "    class_total = torch.zeros(num_classes).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(loader):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "\n",
        "            _, predicted_top1 = torch.max(outputs, 1)\n",
        "            correct_top1 += (predicted_top1 == labels).sum().item()\n",
        "            total_images += labels.size(0)\n",
        "\n",
        "            # Track per-class accuracy\n",
        "            for label, prediction in zip(labels, predicted_top1):\n",
        "                class_correct[label] += (prediction == label).item()\n",
        "                class_total[label] += 1\n",
        "\n",
        "    # Calculate average accuracy per class\n",
        "    avg_class_accuracy = (class_correct / class_total).mean().item() * 100\n",
        "    overall_accuracy = (correct_top1 / total_images) * 100\n",
        "\n",
        "    return running_loss / len(loader.dataset), overall_accuracy, avg_class_accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-L5cMpi-rZQ",
        "outputId": "a93d0376-df49-4cb4-deef-bdd21e22ebcf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 302/302 [07:47<00:00,  1.55s/it]\n",
            "100%|██████████| 76/76 [00:20<00:00,  3.63it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 5.0524 | Validation Loss: 4.2771 | Train Accuracy: 6.21% | Validation Accuracy: 25.75%\n",
            "Model saved!\n",
            "Epoch 2/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 302/302 [09:31<00:00,  1.89s/it]\n",
            "100%|██████████| 76/76 [00:20<00:00,  3.69it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 3.5944 | Validation Loss: 2.7058 | Train Accuracy: 32.62% | Validation Accuracy: 53.24%\n",
            "Model saved!\n",
            "Epoch 3/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 302/302 [09:27<00:00,  1.88s/it]\n",
            "100%|██████████| 76/76 [00:20<00:00,  3.64it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 2.2969 | Validation Loss: 1.8102 | Train Accuracy: 55.79% | Validation Accuracy: 64.70%\n",
            "Model saved!\n",
            "Epoch 4/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 302/302 [09:24<00:00,  1.87s/it]\n",
            "100%|██████████| 76/76 [00:21<00:00,  3.47it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 1.4428 | Validation Loss: 1.3504 | Train Accuracy: 72.71% | Validation Accuracy: 71.10%\n",
            "Model saved!\n",
            "Epoch 5/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 302/302 [09:22<00:00,  1.86s/it]\n",
            "100%|██████████| 76/76 [00:20<00:00,  3.70it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.9192 | Validation Loss: 1.0778 | Train Accuracy: 83.64% | Validation Accuracy: 74.42%\n",
            "Model saved!\n",
            "Epoch 6/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 302/302 [09:26<00:00,  1.88s/it]\n",
            "100%|██████████| 76/76 [00:21<00:00,  3.46it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.6180 | Validation Loss: 1.0221 | Train Accuracy: 90.83% | Validation Accuracy: 76.16%\n",
            "Model saved!\n",
            "Epoch 7/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 302/302 [09:27<00:00,  1.88s/it]\n",
            "100%|██████████| 76/76 [00:22<00:00,  3.40it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.5617 | Validation Loss: 1.0029 | Train Accuracy: 92.32% | Validation Accuracy: 76.08%\n",
            "Epoch 8/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 302/302 [09:30<00:00,  1.89s/it]\n",
            "100%|██████████| 76/76 [00:31<00:00,  2.40it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.5149 | Validation Loss: 0.9810 | Train Accuracy: 93.08% | Validation Accuracy: 77.41%\n",
            "Model saved!\n",
            "Epoch 9/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 302/302 [09:29<00:00,  1.89s/it]\n",
            "100%|██████████| 76/76 [00:51<00:00,  1.46it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.4828 | Validation Loss: 0.9608 | Train Accuracy: 94.08% | Validation Accuracy: 78.07%\n",
            "Model saved!\n",
            "Epoch 10/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 302/302 [09:33<00:00,  1.90s/it]\n",
            "100%|██████████| 76/76 [00:27<00:00,  2.81it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 0.4553 | Validation Loss: 0.9441 | Train Accuracy: 94.49% | Validation Accuracy: 78.49%\n",
            "Model saved!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "\n",
        "# 7. Training Loop with Early Stopping and Class Accuracy\n",
        "num_epochs = 10\n",
        "best_acc = 0.0\n",
        "early_stopper = EarlyStopping(patience=5)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "    train_loss, train_acc = train(model, train_loader, criterion, optimizer, device)\n",
        "    val_loss, val_acc, avg_class_acc = evaluate(model, test_loader, criterion, device, num_classes=200)\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    print(f\"Train Loss: {train_loss:.4f} | Validation Loss: {val_loss:.4f} | Train Accuracy: {train_acc:.2f}% | Validation Accuracy: {val_acc:.2f}%\")\n",
        "    print(f\"Average Accuracy per Class: {avg_class_acc:.2f}%\")\n",
        "\n",
        "    # Save the best model\n",
        "    if val_acc > best_acc:\n",
        "        best_acc = val_acc\n",
        "        torch.save(model.state_dict(), 'best_model.pth')\n",
        "        print(\"Model saved!\")\n",
        "\n",
        "    # Early stopping check\n",
        "    early_stopper(val_loss)\n",
        "    if early_stopper.early_stop:\n",
        "        print(\"Early stopping triggered!\")\n",
        "        break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
